{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7412dad-223a-483f-bd5e-4e609e02349f",
   "metadata": {},
   "source": [
    "# Test Tensorflow-federated (TFF) library\n",
    "\n",
    "## Test #4 : custom `ClientWork` process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff905805-6df3-45c2-a9cd-410a7905d1e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-22 15:48:36.895982: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-22 15:48:36.990599: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-11-22 15:48:37.014411: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "/etinfo/users2/qlanglois/.local/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version : 2.10.0\n",
      "Tensorflow-federated version : 0.39.0\n",
      "# GPUs : 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-22 15:48:47.511167: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-22 15:48:47.906341: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 371 MB memory:  -> device: 0, name: Quadro RTX 5000, pci bus id: 0000:17:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "b'Hello, World!'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import collections\n",
    "import nest_asyncio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_federated as tff\n",
    "\n",
    "tf.config.set_visible_devices([tf.config.list_physical_devices('GPU')[0]], 'GPU')\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "print('Tensorflow version : {}'.format(tf.__version__))\n",
    "print('Tensorflow-federated version : {}'.format(tff.__version__))\n",
    "print('# GPUs : {}'.format(len(tf.config.list_logical_devices('GPU'))))\n",
    "\n",
    "tff.federated_computation(lambda: 'Hello, World!')()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26c473d2-9e4a-4f98-ba6a-de2f36e72665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Summary of model ==========\n",
      "\n",
      "Model: \"multi_layer_perceptron\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " dense_0 (Dense)             (None, 32)                25120     \n",
      "                                                                 \n",
      " activation (Activation)     (None, 32)                0         \n",
      "                                                                 \n",
      " classification_layer (Dense  (None, 10)               330       \n",
      " )                                                               \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 10)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 25,450\n",
      "Trainable params: 25,450\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "\n",
      "========== test_fl_4 ==========\n",
      "Sub model model\n",
      "- Inputs \t: (None, 28, 28, 1)\n",
      "- Outputs \t: (None, 10)\n",
      "- Number of layers \t: 5\n",
      "- Number of parameters \t: 0.025 Millions\n",
      "- Model not compiled\n",
      "\n",
      "Already trained on 0 epochs (0 steps)\n",
      "\n",
      "- Image size : (28, 28, 1)\n",
      "- Normalization style : None\n",
      "- # labels : 10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import tensorflow as tf\n",
    "\n",
    "from models.interfaces.base_fl_model import BaseFLModel\n",
    "from models.interfaces.base_image_model import BaseImageModel\n",
    "\n",
    "class MNISTFLClassifier(BaseImageModel, BaseFLModel):\n",
    "    def __init__(self, input_size = (28, 28, 1), n_labels = 10, ** kwargs):\n",
    "        self._init_image(input_size = input_size, ** kwargs)\n",
    "        self._init_fl(** kwargs)\n",
    "        \n",
    "        self.n_labels = n_labels\n",
    "        \n",
    "        super().__init__(** kwargs)\n",
    "    \n",
    "    def _build_model(self):\n",
    "        super()._build_model(model = {\n",
    "            'architecture_name' : 'perceptron',\n",
    "            'input_shape' : self.input_size,\n",
    "            'units'       : 32,\n",
    "            'n_dense'     : 1,\n",
    "            'activation'  : 'relu',\n",
    "            'drop_rate'   : 0.,\n",
    "            'bnorm'       : 'never',\n",
    "            'output_shape' : self.n_labels,\n",
    "            'final_bias'   : True,\n",
    "            'final_activation' : 'softmax'\n",
    "        })\n",
    "    \n",
    "    @property\n",
    "    def output_signature(self):\n",
    "        return tf.TensorSpec(shape = (None, 1), dtype = tf.int32)\n",
    "    \n",
    "    def __str__(self):\n",
    "        des = super().__str__()\n",
    "        des += self._str_image()\n",
    "        des += self._str_fl()\n",
    "        des += '- # labels : {}\\n'.format(self.n_labels)\n",
    "        return des\n",
    "\n",
    "    def compile(self, loss = 'sparse_categorical_crossentropy', metrics = ['sparse_categorical_accuracy'], ** kwargs):\n",
    "        super().compile(loss = loss, metrics = metrics, ** kwargs)\n",
    "    \n",
    "    def preprocess_data(self, data):\n",
    "        return (\n",
    "            tf.expand_dims(data['pixels'], axis = -1),\n",
    "            tf.cast(tf.reshape(data['label'], [-1, 1]), tf.int32)\n",
    "        )\n",
    "    \n",
    "    def get_dataset_config(self, * args, ** kwargs):\n",
    "        kwargs['batch_before_map'] = True\n",
    "        return super().get_dataset_config(* args, ** kwargs)\n",
    "    \n",
    "    def get_config(self, * args, ** kwargs):\n",
    "        config = super().get_config(* args, ** kwargs)\n",
    "        config.update({\n",
    "            ** self.get_config_image(),\n",
    "            ** self.get_config_fl(),\n",
    "            'n_labels' : self.n_labels\n",
    "        })\n",
    "        return config\n",
    "    \n",
    "    \n",
    "model = MNISTFLClassifier(nom = 'test_fl_4')\n",
    "model.summary()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0d3b094-e516-4177-83fa-7382824a94a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length :\n",
      "  Train length : 3383\n",
      "  Valid length : 3383\n",
      "Data signature : OrderedDict([('label', TensorSpec(shape=(), dtype=tf.int32, name=None)), ('pixels', TensorSpec(shape=(28, 28), dtype=tf.float32, name=None))])\n"
     ]
    }
   ],
   "source": [
    "emnist_train, emnist_valid = tff.simulation.datasets.emnist.load_data()\n",
    "print('Dataset length :\\n  Train length : {}\\n  Valid length : {}'.format(\n",
    "    len(emnist_train.client_ids), len(emnist_valid.client_ids)\n",
    "))\n",
    "print('Data signature : {}'.format(emnist_train.element_type_structure))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217e6656-c0e9-4869-a5a8-85f40dfee3e9",
   "metadata": {},
   "source": [
    "## Model + process initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a03e6ae-f22f-4382-b914-499c9169cd1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_fn': <function models.interfaces.base_fl_model.BaseFLModel.get_model_fn.<locals>.vanilla_model_fn()>,\n",
       " 'loss_fn': <function models.interfaces.base_fl_model.BaseFLModel.get_loss_fn.<locals>.<lambda>()>,\n",
       " 'metrics_fn': <function models.interfaces.base_fl_model.BaseFLModel.get_metrics_fn.<locals>.<lambda>()>,\n",
       " 'server_optimizer_fn': <function models.interfaces.base_fl_model.BaseFLModel.get_optimizer_fn.<locals>.<lambda>()>,\n",
       " 'client_optimizer_fn': <function models.interfaces.base_fl_model.BaseFLModel.get_optimizer_fn.<locals>.<lambda>()>,\n",
       " 'reconstruction_optimizer_fn': <function models.interfaces.base_fl_model.BaseFLModel.get_optimizer_fn.<locals>.<lambda>()>,\n",
       " 'train_ids': ['f1491_42',\n",
       "  'f3914_30',\n",
       "  'f0724_37',\n",
       "  'f1546_05',\n",
       "  'f3912_15',\n",
       "  'f1382_07',\n",
       "  'f2225_86',\n",
       "  'f0932_44',\n",
       "  'f1728_02',\n",
       "  'f2165_54'],\n",
       " 'valid_ids': ['f1491_42', 'f3914_30', 'f0724_37', 'f1546_05', 'f3912_15'],\n",
       " 'x': [<PrefetchDataset element_spec=(TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None, 1), dtype=tf.int32, name=None))>,\n",
       "  <PrefetchDataset element_spec=(TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None, 1), dtype=tf.int32, name=None))>,\n",
       "  <PrefetchDataset element_spec=(TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None, 1), dtype=tf.int32, name=None))>,\n",
       "  <PrefetchDataset element_spec=(TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None, 1), dtype=tf.int32, name=None))>,\n",
       "  <PrefetchDataset element_spec=(TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None, 1), dtype=tf.int32, name=None))>,\n",
       "  <PrefetchDataset element_spec=(TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None, 1), dtype=tf.int32, name=None))>,\n",
       "  <PrefetchDataset element_spec=(TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None, 1), dtype=tf.int32, name=None))>,\n",
       "  <PrefetchDataset element_spec=(TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None, 1), dtype=tf.int32, name=None))>,\n",
       "  <PrefetchDataset element_spec=(TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None, 1), dtype=tf.int32, name=None))>,\n",
       "  <PrefetchDataset element_spec=(TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None, 1), dtype=tf.int32, name=None))>],\n",
       " 'epochs': 5,\n",
       " 'verbose': 1,\n",
       " 'callbacks': [<custom_train_objects.callbacks.terminate_on_nan.TerminateOnNaN at 0x7f7159affc70>,\n",
       "  <custom_train_objects.callbacks.ckpt_callback.CkptCallback at 0x7f7159aff1c0>],\n",
       " 'validation_data': [<PrefetchDataset element_spec=(TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None, 1), dtype=tf.int32, name=None))>,\n",
       "  <PrefetchDataset element_spec=(TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None, 1), dtype=tf.int32, name=None))>,\n",
       "  <PrefetchDataset element_spec=(TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None, 1), dtype=tf.int32, name=None))>,\n",
       "  <PrefetchDataset element_spec=(TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None, 1), dtype=tf.int32, name=None))>,\n",
       "  <PrefetchDataset element_spec=(TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None, 1), dtype=tf.int32, name=None))>],\n",
       " 'shuffle': False,\n",
       " 'initial_epoch': 0,\n",
       " 'global_batch_size': 16}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile()\n",
    "config = model._get_fl_train_config(\n",
    "    emnist_train, validation_data = emnist_valid, n_train_clients = 10, n_valid_clients = 5, shuffle_size = 0\n",
    ")\n",
    "\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6575d949-d157-490b-b456-4d3f52a5d9ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<\n",
      "  state=<>@SERVER,\n",
      "  value=<\n",
      "    trainable=<\n",
      "      float32[784,32],\n",
      "      float32[32],\n",
      "      float32[32,10],\n",
      "      float32[10]\n",
      "    >,\n",
      "    non_trainable=<>\n",
      "  >@SERVER\n",
      "> -> <\n",
      "  state=<>@SERVER,\n",
      "  result=<\n",
      "    trainable=<\n",
      "      float32[784,32],\n",
      "      float32[32],\n",
      "      float32[32,10],\n",
      "      float32[10]\n",
      "    >,\n",
      "    non_trainable=<>\n",
      "  >@CLIENTS,\n",
      "  measurements=<>@SERVER\n",
      ">)\n",
      "\n",
      "(<\n",
      "  state=<>@SERVER,\n",
      "  model_weights={<\n",
      "    trainable=<\n",
      "      float32[784,32],\n",
      "      float32[32],\n",
      "      float32[32,10],\n",
      "      float32[10]\n",
      "    >,\n",
      "    non_trainable=<>\n",
      "  >}@CLIENTS,\n",
      "  client_dataset_with_id={<\n",
      "    int32*,\n",
      "    <\n",
      "      float32[?,28,28,1],\n",
      "      int32[?,1]\n",
      "    >*\n",
      "  >}@CLIENTS\n",
      "> -> <\n",
      "  state=<>@SERVER,\n",
      "  result={<\n",
      "    update=<\n",
      "      float32[784,32],\n",
      "      float32[32],\n",
      "      float32[32,10],\n",
      "      float32[10]\n",
      "    >,\n",
      "    update_weight=float32\n",
      "  >}@CLIENTS,\n",
      "  measurements=<\n",
      "    train_metrics=<\n",
      "      sparse_categorical_accuracy=float32,\n",
      "      loss=float32,\n",
      "      num_examples=int64,\n",
      "      num_batches=int64\n",
      "    >,\n",
      "    train_metrics_per_client=<\n",
      "      sparse_categorical_accuracy=float32[?],\n",
      "      loss=float32[?],\n",
      "      num_examples=int64[?],\n",
      "      num_batches=int64[?],\n",
      "      id=int32[?]\n",
      "    >\n",
      "  >@SERVER\n",
      ">)\n",
      "\n",
      "(<\n",
      "  state=<\n",
      "    value_sum_process=<>,\n",
      "    weight_sum_process=<>\n",
      "  >@SERVER,\n",
      "  value={<\n",
      "    float32[784,32],\n",
      "    float32[32],\n",
      "    float32[32,10],\n",
      "    float32[10]\n",
      "  >}@CLIENTS,\n",
      "  weight={float32}@CLIENTS\n",
      "> -> <\n",
      "  state=<\n",
      "    value_sum_process=<>,\n",
      "    weight_sum_process=<>\n",
      "  >@SERVER,\n",
      "  result=<\n",
      "    float32[784,32],\n",
      "    float32[32],\n",
      "    float32[32,10],\n",
      "    float32[10]\n",
      "  >@SERVER,\n",
      "  measurements=<\n",
      "    mean_value=<>,\n",
      "    mean_weight=<>\n",
      "  >@SERVER\n",
      ">)\n",
      "\n",
      "(<\n",
      "  state=<\n",
      "    int64,\n",
      "    float32[784,32],\n",
      "    float32[32],\n",
      "    float32[32,10],\n",
      "    float32[10],\n",
      "    float32[784,32],\n",
      "    float32[32],\n",
      "    float32[32,10],\n",
      "    float32[10]\n",
      "  >@SERVER,\n",
      "  weights=<\n",
      "    trainable=<\n",
      "      float32[784,32],\n",
      "      float32[32],\n",
      "      float32[32,10],\n",
      "      float32[10]\n",
      "    >,\n",
      "    non_trainable=<>\n",
      "  >@SERVER,\n",
      "  update=<\n",
      "    float32[784,32],\n",
      "    float32[32],\n",
      "    float32[32,10],\n",
      "    float32[10]\n",
      "  >@SERVER\n",
      "> -> <\n",
      "  state=<\n",
      "    int64,\n",
      "    float32[784,32],\n",
      "    float32[32],\n",
      "    float32[32,10],\n",
      "    float32[10],\n",
      "    float32[784,32],\n",
      "    float32[32],\n",
      "    float32[32,10],\n",
      "    float32[10]\n",
      "  >@SERVER,\n",
      "  result=<\n",
      "    trainable=<\n",
      "      float32[784,32],\n",
      "      float32[32],\n",
      "      float32[32,10],\n",
      "      float32[10]\n",
      "    >,\n",
      "    non_trainable=<>\n",
      "  >@SERVER,\n",
      "  measurements=<>@SERVER\n",
      ">)\n",
      "\n",
      "( -> <\n",
      "  global_model_weights=<\n",
      "    trainable=<\n",
      "      float32[784,32],\n",
      "      float32[32],\n",
      "      float32[32,10],\n",
      "      float32[10]\n",
      "    >,\n",
      "    non_trainable=<>\n",
      "  >,\n",
      "  distributor=<>,\n",
      "  client_work=<>,\n",
      "  aggregator=<\n",
      "    value_sum_process=<>,\n",
      "    weight_sum_process=<>\n",
      "  >,\n",
      "  finalizer=<\n",
      "    int64,\n",
      "    float32[784,32],\n",
      "    float32[32],\n",
      "    float32[32,10],\n",
      "    float32[10],\n",
      "    float32[784,32],\n",
      "    float32[32],\n",
      "    float32[32,10],\n",
      "    float32[10]\n",
      "  >\n",
      ">@SERVER)\n",
      "(<\n",
      "  state=<\n",
      "    global_model_weights=<\n",
      "      trainable=<\n",
      "        float32[784,32],\n",
      "        float32[32],\n",
      "        float32[32,10],\n",
      "        float32[10]\n",
      "      >,\n",
      "      non_trainable=<>\n",
      "    >,\n",
      "    distributor=<>,\n",
      "    client_work=<>,\n",
      "    aggregator=<\n",
      "      value_sum_process=<>,\n",
      "      weight_sum_process=<>\n",
      "    >,\n",
      "    finalizer=<\n",
      "      int64,\n",
      "      float32[784,32],\n",
      "      float32[32],\n",
      "      float32[32,10],\n",
      "      float32[10],\n",
      "      float32[784,32],\n",
      "      float32[32],\n",
      "      float32[32,10],\n",
      "      float32[10]\n",
      "    >\n",
      "  >@SERVER,\n",
      "  client_data={<\n",
      "    int32*,\n",
      "    <\n",
      "      float32[?,28,28,1],\n",
      "      int32[?,1]\n",
      "    >*\n",
      "  >}@CLIENTS\n",
      "> -> <\n",
      "  state=<\n",
      "    global_model_weights=<\n",
      "      trainable=<\n",
      "        float32[784,32],\n",
      "        float32[32],\n",
      "        float32[32,10],\n",
      "        float32[10]\n",
      "      >,\n",
      "      non_trainable=<>\n",
      "    >,\n",
      "    distributor=<>,\n",
      "    client_work=<>,\n",
      "    aggregator=<\n",
      "      value_sum_process=<>,\n",
      "      weight_sum_process=<>\n",
      "    >,\n",
      "    finalizer=<\n",
      "      int64,\n",
      "      float32[784,32],\n",
      "      float32[32],\n",
      "      float32[32,10],\n",
      "      float32[10],\n",
      "      float32[784,32],\n",
      "      float32[32],\n",
      "      float32[32,10],\n",
      "      float32[10]\n",
      "    >\n",
      "  >@SERVER,\n",
      "  metrics=<\n",
      "    distributor=<>,\n",
      "    client_work=<\n",
      "      train_metrics=<\n",
      "        sparse_categorical_accuracy=float32,\n",
      "        loss=float32,\n",
      "        num_examples=int64,\n",
      "        num_batches=int64\n",
      "      >,\n",
      "      train_metrics_per_client=<\n",
      "        sparse_categorical_accuracy=float32[?],\n",
      "        loss=float32[?],\n",
      "        num_examples=int64[?],\n",
      "        num_batches=int64[?],\n",
      "        id=int32[?]\n",
      "      >\n",
      "    >,\n",
      "    aggregator=<\n",
      "      mean_value=<>,\n",
      "      mean_weight=<>\n",
      "    >,\n",
      "    finalizer=<>\n",
      "  >@SERVER\n",
      ">)\n"
     ]
    }
   ],
   "source": [
    "model_fn = config['model_fn']\n",
    "\n",
    "@tff.tf_computation\n",
    "def initial_model_weights_fn():\n",
    "    return tff.learning.models.ModelWeights.from_model(model_fn())\n",
    "\n",
    "weights_type = initial_model_weights_fn.type_signature.result\n",
    "aggregator_factory = tff.aggregators.MeanFactory()\n",
    "\n",
    "distributor  = tff.learning.templates.build_broadcast_process(weights_type)\n",
    "client_work  = model.build_client_work(config['model_fn'], config['client_optimizer_fn'])\n",
    "aggregator   = aggregator_factory.create(weights_type.trainable, tff.TensorType(tf.float32))\n",
    "finalizer    = tff.learning.templates.build_apply_optimizer_finalizer(config['server_optimizer_fn'], weights_type)\n",
    "\n",
    "training_process = tff.learning.templates.compose_learning_process(\n",
    "    initial_model_weights_fn,\n",
    "    distributor,\n",
    "    client_work,\n",
    "    aggregator,\n",
    "    finalizer\n",
    ") \n",
    "\n",
    "print(distributor.next.type_signature.formatted_representation())\n",
    "print()\n",
    "print(client_work.next.type_signature.formatted_representation())\n",
    "print()\n",
    "print(aggregator.next.type_signature.formatted_representation())\n",
    "print()\n",
    "print(finalizer.next.type_signature.formatted_representation())\n",
    "print()\n",
    "\n",
    "print(training_process.initialize.type_signature.formatted_representation())\n",
    "print(training_process.next.type_signature.formatted_representation())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b60c7a-a51a-4cbd-85b5-d39fe841593b",
   "metadata": {},
   "source": [
    "## Training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53a35675-f59f-4200-a71a-2dd4d0675c32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-22 15:48:54.393266: W tensorflow/core/data/root_dataset.cc:266] Optimization loop failed: CANCELLED: Operation was cancelled\n",
      "2022-11-22 15:48:54.393470: W tensorflow/core/data/root_dataset.cc:266] Optimization loop failed: CANCELLED: Operation was cancelled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train_metrics', OrderedDict([('sparse_categorical_accuracy', 0.06566604), ('loss', 2.6932056), ('num_examples', 533), ('num_batches', 36)])), ('train_metrics_per_client', OrderedDict([('sparse_categorical_accuracy', array([0.04444445, 0.05454545, 0.06034483, 0.10434783, 0.05882353],\n",
      "      dtype=float32)), ('loss', array([2.755179 , 2.7103982, 2.6461573, 2.7398257, 2.620559 ],\n",
      "      dtype=float32)), ('num_examples', array([ 90, 110, 116, 115, 102])), ('num_batches', array([6, 7, 8, 8, 7])), ('id', array([2, 3, 1, 4, 0], dtype=int32))]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', ())])\n"
     ]
    }
   ],
   "source": [
    "train_ids, train_fed_data = config['train_ids'], config['x']\n",
    "valid_ids, valid_fed_data = config['valid_ids'], config['validation_data']\n",
    "\n",
    "state  = training_process.initialize()\n",
    "\n",
    "sample_data = train_fed_data[:5]\n",
    "sample_ids  = train_ids[:5]\n",
    "\n",
    "seq_train_ids = [tf.data.Dataset.from_tensor_slices(np.array([train_ids.index(id_i)], dtype = np.int32)) for id_i in sample_ids]\n",
    "result = training_process.next(state, list(zip(seq_train_ids, sample_data)))\n",
    "print(result.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "715cd801-06c9-4edc-8350-0231fc5d7661",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train_metrics', OrderedDict([('sparse_categorical_accuracy', 0.05882353), ('loss', 2.620559), ('num_examples', 102), ('num_batches', 7)])), ('train_metrics_per_client', OrderedDict([('sparse_categorical_accuracy', array([0.05882353], dtype=float32)), ('loss', array([2.620559], dtype=float32)), ('num_examples', array([102])), ('num_batches', array([7])), ('id', array([0], dtype=int32))]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', ())])\n"
     ]
    }
   ],
   "source": [
    "#state  = training_process.initialize()\n",
    "\n",
    "sample_data = train_fed_data[:1]\n",
    "sample_ids  = train_ids[:1]\n",
    "\n",
    "seq_train_ids = [tf.data.Dataset.from_tensor_slices(np.array([train_ids.index(id_i)], dtype = np.int32)) for id_i in sample_ids]\n",
    "result = training_process.next(state, list(zip(seq_train_ids, sample_data)))\n",
    "print(result.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f254cb96-57bc-4df8-9606-36e6cf8860a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-22 15:50:31.248510: W tensorflow/core/data/root_dataset.cc:266] Optimization loop failed: CANCELLED: Operation was cancelled\n",
      "2022-11-22 15:50:31.264398: W tensorflow/core/data/root_dataset.cc:266] Optimization loop failed: CANCELLED: Operation was cancelled\n",
      "2022-11-22 15:50:31.274428: W tensorflow/core/data/root_dataset.cc:266] Optimization loop failed: CANCELLED: Operation was cancelled\n",
      "2022-11-22 15:50:31.277280: W tensorflow/core/data/root_dataset.cc:266] Optimization loop failed: CANCELLED: Operation was cancelled\n",
      "2022-11-22 15:50:31.321588: W tensorflow/core/data/root_dataset.cc:266] Optimization loop failed: CANCELLED: Operation was cancelled\n",
      "2022-11-22 15:50:31.322021: W tensorflow/core/data/root_dataset.cc:266] Optimization loop failed: CANCELLED: Operation was cancelled\n",
      "2022-11-22 15:50:31.322402: W tensorflow/core/data/root_dataset.cc:266] Optimization loop failed: CANCELLED: Operation was cancelled\n",
      "2022-11-22 15:50:31.322434: W tensorflow/core/data/root_dataset.cc:266] Optimization loop failed: CANCELLED: Operation was cancelled\n",
      "2022-11-22 15:50:31.332317: W tensorflow/core/data/root_dataset.cc:266] Optimization loop failed: CANCELLED: Operation was cancelled\n",
      "2022-11-22 15:50:31.332369: W tensorflow/core/data/root_dataset.cc:266] Optimization loop failed: CANCELLED: Operation was cancelled\n",
      "2022-11-22 15:50:31.332442: W tensorflow/core/data/root_dataset.cc:266] Optimization loop failed: CANCELLED: Operation was cancelled\n",
      "2022-11-22 15:50:31.398614: W tensorflow/core/data/root_dataset.cc:266] Optimization loop failed: CANCELLED: Operation was cancelled\n",
      "2022-11-22 15:50:31.403440: W tensorflow/core/data/root_dataset.cc:266] Optimization loop failed: CANCELLED: Operation was cancelled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train_metrics', OrderedDict([('sparse_categorical_accuracy', 0.07166124), ('loss', 2.7558067), ('num_examples', 2456), ('num_batches', 168)])), ('train_metrics_per_client', OrderedDict([('sparse_categorical_accuracy', array([0.08333334, 0.05882353, 0.06956521, 0.04444445, 0.06818182,\n",
      "       0.04444445, 0.08333334, 0.10434783, 0.06034483, 0.08333334,\n",
      "       0.06818182, 0.06818182, 0.13      , 0.10434783, 0.08163265,\n",
      "       0.06034483, 0.05454545, 0.08163265, 0.06034483, 0.04444445,\n",
      "       0.05454545, 0.06818182, 0.05882353, 0.10434783, 0.04444445],\n",
      "      dtype=float32)), ('loss', array([2.8469965, 2.620559 , 2.7045653, 2.755179 , 2.9184117, 2.755179 ,\n",
      "       2.8469965, 2.7398257, 2.6461573, 2.8469965, 2.9184117, 2.9184117,\n",
      "       2.6834817, 2.7398257, 2.8531463, 2.6461573, 2.7103982, 2.8531463,\n",
      "       2.6461573, 2.755179 , 2.7103982, 2.9184117, 2.620559 , 2.7398257,\n",
      "       2.755179 ], dtype=float32)), ('num_examples', array([ 72, 102, 115,  90,  88,  90,  72, 115, 116,  72,  88,  88, 100,\n",
      "       115,  98, 116, 110,  98, 116,  90, 110,  88, 102, 115,  90])), ('num_batches', array([5, 7, 8, 6, 6, 6, 5, 8, 8, 5, 6, 6, 7, 8, 7, 8, 7, 7, 8, 6, 7, 6,\n",
      "       7, 8, 6])), ('id', array([6, 0, 8, 2, 7, 2, 6, 4, 1, 6, 7, 7, 9, 4, 5, 1, 3, 5, 1, 2, 3, 7,\n",
      "       0, 4, 2], dtype=int32))]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', ())])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-22 15:50:32.395134: W tensorflow/core/data/root_dataset.cc:266] Optimization loop failed: CANCELLED: Operation was cancelled\n",
      "2022-11-22 15:50:32.395954: W tensorflow/core/data/root_dataset.cc:266] Optimization loop failed: CANCELLED: Operation was cancelled\n",
      "2022-11-22 15:50:32.398750: W tensorflow/core/data/root_dataset.cc:266] Optimization loop failed: CANCELLED: Operation was cancelled\n",
      "2022-11-22 15:50:32.419162: W tensorflow/core/data/root_dataset.cc:266] Optimization loop failed: CANCELLED: Operation was cancelled\n",
      "2022-11-22 15:50:32.444696: W tensorflow/core/data/root_dataset.cc:266] Optimization loop failed: CANCELLED: Operation was cancelled\n",
      "2022-11-22 15:50:32.508372: W tensorflow/core/data/root_dataset.cc:266] Optimization loop failed: CANCELLED: Operation was cancelled\n",
      "2022-11-22 15:50:32.508767: W tensorflow/core/data/root_dataset.cc:266] Optimization loop failed: CANCELLED: Operation was cancelled\n",
      "2022-11-22 15:50:32.508854: W tensorflow/core/data/root_dataset.cc:266] Optimization loop failed: CANCELLED: Operation was cancelled\n",
      "2022-11-22 15:50:32.509249: W tensorflow/core/data/root_dataset.cc:266] Optimization loop failed: CANCELLED: Operation was cancelled\n",
      "2022-11-22 15:50:32.511250: W tensorflow/core/data/root_dataset.cc:266] Optimization loop failed: CANCELLED: Operation was cancelled\n",
      "2022-11-22 15:50:32.511581: W tensorflow/core/data/root_dataset.cc:266] Optimization loop failed: CANCELLED: Operation was cancelled\n",
      "2022-11-22 15:50:32.516361: W tensorflow/core/data/root_dataset.cc:266] Optimization loop failed: CANCELLED: Operation was cancelled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train_metrics', OrderedDict([('sparse_categorical_accuracy', 0.07980255), ('loss', 3.101888), ('num_examples', 2431), ('num_batches', 168)])), ('train_metrics_per_client', OrderedDict([('sparse_categorical_accuracy', array([0.06363636, 0.14      , 0.04901961, 0.11363637, 0.14      ,\n",
      "       0.06944445, 0.04310345, 0.04310345, 0.11304348, 0.11363637,\n",
      "       0.06944445, 0.04310345, 0.04901961, 0.14      , 0.06944445,\n",
      "       0.09183674, 0.09183674, 0.04901961, 0.06363636, 0.09183674,\n",
      "       0.06944445, 0.06944445, 0.09183674, 0.09183674, 0.04310345],\n",
      "      dtype=float32)), ('loss', array([3.065065 , 3.1307182, 2.8216481, 3.114571 , 3.1307182, 3.2332153,\n",
      "       3.2468092, 3.2468092, 3.0259469, 3.114571 , 3.2332153, 3.2468092,\n",
      "       2.8216481, 3.1307182, 3.2332153, 3.0570993, 3.0570993, 2.8216481,\n",
      "       3.065065 , 3.0570993, 3.2332153, 3.2332153, 3.0570993, 3.0570993,\n",
      "       3.2468092], dtype=float32)), ('num_examples', array([110, 100, 102,  88, 100,  72, 116, 116, 115,  88,  72, 116, 102,\n",
      "       100,  72,  98,  98, 102, 110,  98,  72,  72,  98,  98, 116])), ('num_batches', array([7, 7, 7, 6, 7, 5, 8, 8, 8, 6, 5, 8, 7, 7, 5, 7, 7, 7, 7, 7, 5, 5,\n",
      "       7, 7, 8])), ('id', array([3, 9, 0, 7, 9, 6, 1, 1, 4, 7, 6, 1, 0, 9, 6, 5, 5, 0, 3, 5, 6, 6,\n",
      "       5, 5, 1], dtype=int32))]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', ())])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-22 15:50:33.579363: W tensorflow/core/data/root_dataset.cc:266] Optimization loop failed: CANCELLED: Operation was cancelled\n",
      "2022-11-22 15:50:33.581370: W tensorflow/core/data/root_dataset.cc:266] Optimization loop failed: CANCELLED: Operation was cancelled\n",
      "2022-11-22 15:50:33.581470: W tensorflow/core/data/root_dataset.cc:266] Optimization loop failed: CANCELLED: Operation was cancelled\n",
      "2022-11-22 15:50:33.582421: W tensorflow/core/data/root_dataset.cc:266] Optimization loop failed: CANCELLED: Operation was cancelled\n",
      "2022-11-22 15:50:33.604064: W tensorflow/core/data/root_dataset.cc:266] Optimization loop failed: CANCELLED: Operation was cancelled\n",
      "2022-11-22 15:50:33.604292: W tensorflow/core/data/root_dataset.cc:266] Optimization loop failed: CANCELLED: Operation was cancelled\n",
      "2022-11-22 15:50:33.604899: W tensorflow/core/data/root_dataset.cc:266] Optimization loop failed: CANCELLED: Operation was cancelled\n",
      "2022-11-22 15:50:33.611019: W tensorflow/core/data/root_dataset.cc:266] Optimization loop failed: CANCELLED: Operation was cancelled\n",
      "2022-11-22 15:50:33.615143: W tensorflow/core/data/root_dataset.cc:266] Optimization loop failed: CANCELLED: Operation was cancelled\n",
      "2022-11-22 15:50:33.615185: W tensorflow/core/data/root_dataset.cc:266] Optimization loop failed: CANCELLED: Operation was cancelled\n",
      "2022-11-22 15:50:33.623592: W tensorflow/core/data/root_dataset.cc:266] Optimization loop failed: CANCELLED: Operation was cancelled\n",
      "2022-11-22 15:50:33.654996: W tensorflow/core/data/root_dataset.cc:266] Optimization loop failed: CANCELLED: Operation was cancelled\n",
      "2022-11-22 15:50:33.655827: W tensorflow/core/data/root_dataset.cc:266] Optimization loop failed: CANCELLED: Operation was cancelled\n",
      "2022-11-22 15:50:33.662582: W tensorflow/core/data/root_dataset.cc:266] Optimization loop failed: CANCELLED: Operation was cancelled\n",
      "2022-11-22 15:50:33.674319: W tensorflow/core/data/root_dataset.cc:266] Optimization loop failed: CANCELLED: Operation was cancelled\n",
      "2022-11-22 15:50:33.674409: W tensorflow/core/data/root_dataset.cc:266] Optimization loop failed: CANCELLED: Operation was cancelled\n",
      "2022-11-22 15:50:33.683707: W tensorflow/core/data/root_dataset.cc:266] Optimization loop failed: CANCELLED: Operation was cancelled\n",
      "2022-11-22 15:50:33.685605: W tensorflow/core/data/root_dataset.cc:266] Optimization loop failed: CANCELLED: Operation was cancelled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train_metrics', OrderedDict([('sparse_categorical_accuracy', 0.09324378), ('loss', 3.521152), ('num_examples', 2531), ('num_batches', 173)])), ('train_metrics_per_client', OrderedDict([('sparse_categorical_accuracy', array([0.11      , 0.06122449, 0.11      , 0.10344828, 0.09090909,\n",
      "       0.09090909, 0.0882353 , 0.10434783, 0.0882353 , 0.11      ,\n",
      "       0.11      , 0.10434783, 0.11      , 0.10227273, 0.06122449,\n",
      "       0.10227273, 0.04166667, 0.09090909, 0.04444445, 0.0882353 ,\n",
      "       0.11      , 0.10434783, 0.09090909, 0.0882353 , 0.10227273],\n",
      "      dtype=float32)), ('loss', array([3.6842692, 3.4150226, 3.6842692, 3.5835795, 3.5694823, 3.5694823,\n",
      "       3.2044475, 3.486783 , 3.2044475, 3.6842692, 3.6842692, 3.486783 ,\n",
      "       3.6842692, 3.5814264, 3.4150226, 3.5814264, 3.9203625, 3.5694823,\n",
      "       3.4197135, 3.2044475, 3.6842692, 3.486783 , 3.5694823, 3.2044475,\n",
      "       3.5814264], dtype=float32)), ('num_examples', array([100,  98, 100, 116, 110, 110, 102, 115, 102, 100, 100, 115, 100,\n",
      "        88,  98,  88,  72, 110,  90, 102, 100, 115, 110, 102,  88])), ('num_batches', array([7, 7, 7, 8, 7, 7, 7, 8, 7, 7, 7, 8, 7, 6, 7, 6, 5, 7, 6, 7, 7, 8,\n",
      "       7, 7, 6])), ('id', array([9, 5, 9, 1, 3, 3, 0, 8, 0, 9, 9, 8, 9, 7, 5, 7, 6, 3, 2, 0, 9, 8,\n",
      "       3, 0, 7], dtype=int32))]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', ())])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-22 15:50:34.743556: W tensorflow/core/data/root_dataset.cc:266] Optimization loop failed: CANCELLED: Operation was cancelled\n",
      "2022-11-22 15:50:34.781429: W tensorflow/core/data/root_dataset.cc:266] Optimization loop failed: CANCELLED: Operation was cancelled\n",
      "2022-11-22 15:50:34.783239: W tensorflow/core/data/root_dataset.cc:266] Optimization loop failed: CANCELLED: Operation was cancelled\n",
      "2022-11-22 15:50:34.804585: W tensorflow/core/data/root_dataset.cc:266] Optimization loop failed: CANCELLED: Operation was cancelled\n",
      "2022-11-22 15:50:34.817290: W tensorflow/core/data/root_dataset.cc:266] Optimization loop failed: CANCELLED: Operation was cancelled\n",
      "2022-11-22 15:50:34.818270: W tensorflow/core/data/root_dataset.cc:266] Optimization loop failed: CANCELLED: Operation was cancelled\n",
      "2022-11-22 15:50:34.831248: W tensorflow/core/data/root_dataset.cc:266] Optimization loop failed: CANCELLED: Operation was cancelled\n",
      "2022-11-22 15:50:34.835915: W tensorflow/core/data/root_dataset.cc:266] Optimization loop failed: CANCELLED: Operation was cancelled\n",
      "2022-11-22 15:50:34.844530: W tensorflow/core/data/root_dataset.cc:266] Optimization loop failed: CANCELLED: Operation was cancelled\n",
      "2022-11-22 15:50:34.866309: W tensorflow/core/data/root_dataset.cc:266] Optimization loop failed: CANCELLED: Operation was cancelled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train_metrics', OrderedDict([('sparse_categorical_accuracy', 0.09233177), ('loss', 4.168894), ('num_examples', 2556), ('num_batches', 176)])), ('train_metrics_per_client', OrderedDict([('sparse_categorical_accuracy', array([0.16521738, 0.08695652, 0.06363636, 0.0775862 , 0.10784314,\n",
      "       0.08695652, 0.06363636, 0.09090909, 0.05      , 0.02777778,\n",
      "       0.02777778, 0.08695652, 0.0775862 , 0.16521738, 0.09183674,\n",
      "       0.08695652, 0.16521738, 0.08695652, 0.09090909, 0.08695652,\n",
      "       0.02777778, 0.10784314, 0.16521738, 0.02777778, 0.09090909],\n",
      "      dtype=float32)), ('loss', array([3.9367704, 4.06471  , 4.1661496, 4.2371135, 3.7112954, 4.06471  ,\n",
      "       4.1661496, 4.37812  , 4.359247 , 4.867237 , 4.867237 , 4.06471  ,\n",
      "       4.2371135, 3.9367704, 4.052873 , 4.06471  , 3.9367704, 4.06471  ,\n",
      "       4.37812  , 4.06471  , 4.867237 , 3.7112954, 3.9367704, 4.867237 ,\n",
      "       4.37812  ], dtype=float32)), ('num_examples', array([115, 115, 110, 116, 102, 115, 110,  88, 100,  72,  72, 115, 116,\n",
      "       115,  98, 115, 115, 115,  88, 115,  72, 102, 115,  72,  88])), ('num_batches', array([8, 8, 7, 8, 7, 8, 7, 6, 7, 5, 5, 8, 8, 8, 7, 8, 8, 8, 6, 8, 5, 7,\n",
      "       8, 5, 6])), ('id', array([4, 8, 3, 1, 0, 8, 3, 7, 9, 6, 6, 8, 1, 4, 5, 8, 4, 8, 7, 8, 6, 0,\n",
      "       4, 6, 7], dtype=int32))]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', ())])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-22 15:50:36.114235: W tensorflow/core/data/root_dataset.cc:266] Optimization loop failed: CANCELLED: Operation was cancelled\n",
      "2022-11-22 15:50:36.121365: W tensorflow/core/data/root_dataset.cc:266] Optimization loop failed: CANCELLED: Operation was cancelled\n",
      "2022-11-22 15:50:36.123749: W tensorflow/core/data/root_dataset.cc:266] Optimization loop failed: CANCELLED: Operation was cancelled\n",
      "2022-11-22 15:50:36.129357: W tensorflow/core/data/root_dataset.cc:266] Optimization loop failed: CANCELLED: Operation was cancelled\n",
      "2022-11-22 15:50:36.176882: W tensorflow/core/data/root_dataset.cc:266] Optimization loop failed: CANCELLED: Operation was cancelled\n",
      "2022-11-22 15:50:36.192763: W tensorflow/core/data/root_dataset.cc:266] Optimization loop failed: CANCELLED: Operation was cancelled\n",
      "2022-11-22 15:50:36.194101: W tensorflow/core/data/root_dataset.cc:266] Optimization loop failed: CANCELLED: Operation was cancelled\n",
      "2022-11-22 15:50:36.204415: W tensorflow/core/data/root_dataset.cc:266] Optimization loop failed: CANCELLED: Operation was cancelled\n",
      "2022-11-22 15:50:36.205849: W tensorflow/core/data/root_dataset.cc:266] Optimization loop failed: CANCELLED: Operation was cancelled\n",
      "2022-11-22 15:50:36.208082: W tensorflow/core/data/root_dataset.cc:266] Optimization loop failed: CANCELLED: Operation was cancelled\n",
      "2022-11-22 15:50:36.214234: W tensorflow/core/data/root_dataset.cc:266] Optimization loop failed: CANCELLED: Operation was cancelled\n",
      "2022-11-22 15:50:36.214382: W tensorflow/core/data/root_dataset.cc:266] Optimization loop failed: CANCELLED: Operation was cancelled\n",
      "2022-11-22 15:50:36.215478: W tensorflow/core/data/root_dataset.cc:266] Optimization loop failed: CANCELLED: Operation was cancelled\n",
      "2022-11-22 15:50:36.216147: W tensorflow/core/data/root_dataset.cc:266] Optimization loop failed: CANCELLED: Operation was cancelled\n",
      "2022-11-22 15:50:36.219185: W tensorflow/core/data/root_dataset.cc:266] Optimization loop failed: CANCELLED: Operation was cancelled\n",
      "2022-11-22 15:50:36.223914: W tensorflow/core/data/root_dataset.cc:266] Optimization loop failed: CANCELLED: Operation was cancelled\n",
      "2022-11-22 15:50:36.230294: W tensorflow/core/data/root_dataset.cc:266] Optimization loop failed: CANCELLED: Operation was cancelled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train_metrics', OrderedDict([('sparse_categorical_accuracy', 0.09796748), ('loss', 5.101942), ('num_examples', 2460), ('num_batches', 169)])), ('train_metrics_per_client', OrderedDict([('sparse_categorical_accuracy', array([0.11111111, 0.1       , 0.11304348, 0.10434783, 0.11818182,\n",
      "       0.11111111, 0.07777778, 0.07777778, 0.07777778, 0.0775862 ,\n",
      "       0.12244898, 0.1       , 0.0882353 , 0.11111111, 0.11304348,\n",
      "       0.10434783, 0.10434783, 0.0882353 , 0.0882353 , 0.11111111,\n",
      "       0.07777778, 0.10434783, 0.0882353 , 0.09090909, 0.0882353 ],\n",
      "      dtype=float32)), ('loss', array([6.1724434, 5.304197 , 4.781226 , 4.736877 , 4.934802 , 6.1724434,\n",
      "       5.4446654, 5.4446654, 5.4446654, 5.1820602, 4.729478 , 5.304197 ,\n",
      "       4.704996 , 6.1724434, 4.781226 , 4.736877 , 4.736877 , 4.704996 ,\n",
      "       4.704996 , 6.1724434, 5.4446654, 4.736877 , 4.704996 , 5.4329977,\n",
      "       4.704996 ], dtype=float32)), ('num_examples', array([ 72, 100, 115, 115, 110,  72,  90,  90,  90, 116,  98, 100, 102,\n",
      "        72, 115, 115, 115, 102, 102,  72,  90, 115, 102,  88, 102])), ('num_batches', array([5, 7, 8, 8, 7, 5, 6, 6, 6, 8, 7, 7, 7, 5, 8, 8, 8, 7, 7, 5, 6, 8,\n",
      "       7, 6, 7])), ('id', array([6, 9, 8, 4, 3, 6, 2, 2, 2, 1, 5, 9, 0, 6, 8, 4, 4, 0, 0, 6, 2, 4,\n",
      "       0, 7, 0], dtype=int32))]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', ())])\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "\n",
    "train_ids, train_fed_data = config['train_ids'], config['x']\n",
    "valid_ids, valid_fed_data = config['valid_ids'], config['validation_data']\n",
    "\n",
    "state  = training_process.initialize()\n",
    "\n",
    "for _ in range(epochs):\n",
    "    indexes     = np.random.choice(len(train_fed_data), size = 25)\n",
    "    sample_data = [train_fed_data[idx] for idx in indexes]\n",
    "    sample_ids  = [train_ids[idx] for idx in indexes]\n",
    "\n",
    "    seq_train_ids = [tf.data.Dataset.from_tensor_slices(np.array([train_ids.index(id_i)], dtype = np.int32)) for id_i in sample_ids]\n",
    "    result = training_process.next(state, list(zip(seq_train_ids, sample_data)))\n",
    "    state, metrics = result.state, result.metrics\n",
    "    print(result.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ce062d-08b8-4ff5-8ad2-58e32e7a5ed7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
